{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Task_sentiment analysis .ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Kg52dVNN_6Ue"
      },
      "source": [
        "#Data reading and Preprocessing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wYWVHrCsvSBV"
      },
      "source": [
        "*Download and convert dataset format *\n",
        "we downloaded the yelp dataset using wget modeul as we install it using %pip on local terminal to download it\n",
        "\n",
        "\n",
        "\n",
        "1.   split data set into training and testing\n",
        "2.   write data to file and process the reviews\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sjZotSPrwMvl",
        "outputId": "63628545-af13-4cb8-c9e8-bbc72010ebf7"
      },
      "source": [
        "# WE DON'T RUN THIS CELL IF WE HAVE OUR DATA FILE READY (UNCOMMENT AND RUN IF YOU NEED TO RECREATE THE DATA ..BUT DOWNLOAD ALL 'yelp dataset' first )\n",
        "\"\"\"\n",
        "##Pre-Processing \n",
        "import collections\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "raw_train_dataset_csv=r\"data/yelp/raw_train.csv\"\n",
        "raw_test_dataset_csv=r\"data/yelp/raw_test.csv\"\n",
        "proportion_subset_of_train=0.1\n",
        "train_proportion=0.8\n",
        "test_proportion=0.2\n",
        "output_csv=r\"data/yelp/reviews_with_splits_lite.csv\"\n",
        "seed=1337\n",
        "\n",
        "\n",
        "train_reviews = pd.read_csv(raw_train_dataset_csv, header=None, names=['rating', 'review'])\n",
        "print(train_reviews.head())\n",
        "\n",
        "by_rating = collections.defaultdict(list)\n",
        "for _, row in train_reviews.iterrows():\n",
        "    by_rating[row.rating].append(row.to_dict())\n",
        "    # print(row.to_dict())\n",
        "    # break\n",
        "    \n",
        "review_subset = []\n",
        "\n",
        "for _ , item_list in sorted(by_rating.items()):\n",
        "\n",
        "    n_total = len(item_list)\n",
        "    n_subset = int(proportion_subset_of_train * n_total)\n",
        "    review_subset.extend(item_list[:n_subset])\n",
        "    # print(f\"at { _ } , n total ={n_total} , n subset ={n_subset} ,\\n\")# item list ={item_list}\\n=====\\n\\n\")\n",
        "    \n",
        "\n",
        "review_subset = pd.DataFrame(review_subset)\n",
        "\n",
        "by_rating = collections.defaultdict(list)\n",
        "for _, row in review_subset.iterrows():\n",
        "    by_rating[row.rating].append(row.to_dict())\n",
        "    \n",
        "final_list = []\n",
        "np.random.seed(seed)\n",
        "\n",
        "for _, item_list in sorted(by_rating.items()):\n",
        "\n",
        "    out = self.layer1()\n",
        "    np.random.shuffle(item_list)\n",
        "    \n",
        "    n_total = len(item_list)\n",
        "    n_train = int(train_proportion * n_total)\n",
        "    n_test = int(test_proportion * n_total)\n",
        "    \n",
        "    # Give data point a split attribute\n",
        "    for item in item_list[:n_train]:\n",
        "        item['split'] = 'train'\n",
        "  \n",
        "    for item in item_list[n_train:n_train+n_test]:\n",
        "        item['split'] = 'test'\n",
        "\n",
        "    # Add to final list\n",
        "    final_list.extend(item_list)\n",
        "\n",
        "\n",
        "final_reviews = pd.DataFrame(final_list)\n",
        "def preprocess_text(text):\n",
        "    text = text.lower()\n",
        "    text = re.sub(r\"([.,!?])\", r\" \\1 \", text)\n",
        "    text = re.sub(r\"[^a-zA-Z.,!?]+\", r\" \", text)\n",
        "    return text\n",
        "    \n",
        "final_reviews.review = final_reviews.review.apply(preprocess_text)\n",
        "final_reviews['rating'] = final_reviews.rating.apply({1: -1, 2: 1}.get)\n",
        "final_reviews.to_csv(r\"data/yelp/reviews_with_splits_lite_new.csv\", index=False)\n",
        "\n",
        "\"\"\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   rating                                             review\n",
            "0       1  Unfortunately, the frustration of being Dr. Go...\n",
            "1       2  Been going to Dr. Goldberg for over 10 years. ...\n",
            "2       1  I don't know what Dr. Goldberg was like before...\n",
            "3       1  I'm writing this review to give you a heads up...\n",
            "4       2  All the food is great here. But the best thing...\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3U17XZxDAAW_"
      },
      "source": [
        "## sloution starts from here"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yTXf-6R81weF"
      },
      "source": [
        "Here we implement our own implementation for NN and SVM classifier ( please note that we interrupted the tests because we didn't have enough time )\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FMjWeJ5LxKs8"
      },
      "source": [
        "##implement function for one hot encodeing"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TExN60aPw4li"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MbOup4-12D96"
      },
      "source": [
        "in the next cell we implement the get_one_hot function .it takes review raw and convert it to one hot list with size equals vocabulary size "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AwvWxvlFAHYI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7a8808bd-8b2c-4752-f8cf-14e3c69e548f"
      },
      "source": [
        "import collections\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sklearn \n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "\n",
        "from random import  shuffle \n",
        "\n",
        "DATA_PATH = r'data/yelp/reviews_with_splits_lite_new.csv'\n",
        "\n",
        "AllData = pd.read_csv(DATA_PATH)\n",
        "print(AllData.split.value_counts())\n",
        "print(AllData.head())\n",
        "\n",
        "\n",
        "my_vocab =[]\n",
        "word2id ={}\n",
        "id2word ={}\n",
        "\n",
        "\n",
        "def prepare():\n",
        "  \"\"\" read all reviews and add every word of all reviews to the \n",
        "  vocabulary 'word2id' and 'id2word' dictionaries \"\"\"\n",
        "  for review in AllData['review']:\n",
        "    for word in review.split(\" \"):\n",
        "      if word not in word2id :\n",
        "        index = len(word2id)\n",
        "        word2id[word]=index\n",
        "        id2word[index]= word\n",
        "        \n",
        "\n",
        "def get_one_hot(review):\n",
        "        \"\"\"Create a collapsed one-hot vector for the review\n",
        "        Args:\n",
        "            review (str): the review \n",
        "        Returns:\n",
        "            one_hot (np.ndarray): the collapsed one-hot encoding \n",
        "        \"\"\"\n",
        "        one_hot = np.zeros(len(word2id), dtype=np.float32)\n",
        "        for token in review.split(\" \"):\n",
        "                one_hot[word2id[token]] = 1\n",
        "        return one_hot\n",
        "\n",
        "def transform_df(review_df):\n",
        "  one_hot=np.empty([len(review_df) , len(word2id)])\n",
        "  for i in range(len(review_df)):\n",
        "    one_hot[i] =(get_one_hot(review_df[i]))\n",
        "  one_hot_df = pd.DataFrame()\n",
        "  one_hot_df['review'] =pd.Series(one_hot)\n",
        "  return one_hot_df\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "train    44800\n",
            "test     11200\n",
            "Name: split, dtype: int64\n",
            "   rating                                             review  split\n",
            "0      -1  terrible place to work for i just heard a stor...  train\n",
            "1      -1   hours , minutes total time for an extremely s...  train\n",
            "2      -1  my less than stellar review is for service . w...  train\n",
            "3      -1  i m granting one star because there s no way t...  train\n",
            "4      -1  the food here is mediocre at best . i went aft...  train\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ObEX7xWpxZzy"
      },
      "source": [
        "###Download the pretrained Glove embedding from to stanford and process the .txt file into numpy array"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CsduCb9H2pip"
      },
      "source": [
        "After downloading Glove file and proccess it . we implemented get_Glove_vec function to take the review raw and return its Glove embedding by the averageg of its words "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "bThZE3V_kdka",
        "outputId": "a30f9e5e-b738-4083-f16f-a82371a74fc5"
      },
      "source": [
        "#download Glove\n",
        "# !wget http://nlp.stanford.edu/data/glove.6B.zip\n",
        "# !unzip glove*.zip\n",
        "gloveModel = {}\n",
        "f = open('glove.6B.100d.txt', encoding='utf-8')\n",
        "\n",
        "print(\"Loading Glove Model\")\n",
        "for line in f:\n",
        "  splitLines = line.split()\n",
        "  word = splitLines[0]\n",
        "  wordEmbedding = np.array([float(value) for value in splitLines[1:]])\n",
        "  gloveModel[word] = wordEmbedding\n",
        "\n",
        "def get_Glove_vec(sent):\n",
        "  embed = {}\n",
        "  words=sent.split()\n",
        "  sent_vect=(np.mean([gloveModel[word] for word in words], axis=0)).tolist()\n",
        "  return Glove_vec\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading Glove Model\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W2afC565yECp"
      },
      "source": [
        "in the following line we assign the data into X and Y "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q-eCOs2wmbu0"
      },
      "source": [
        "\n",
        "prepare()\n",
        "dtrain = AllData[AllData['split']=='train']\n",
        "dtest = AllData[AllData['split']=='test']\n",
        "\n",
        "xtrain , ytrain = dtrain['review'] , dtrain['rating']\n",
        "xtest  , ytest = dtest['review'] , dtest['rating']\n",
        "xsample , ysample = xtrain[:20] , ytrain[:20]\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-6HV_EBtgYcr",
        "outputId": "682a6347-076f-41f5-cf02-201714ed45b5"
      },
      "source": [
        "print(xtrain.shape)\n",
        "try :\n",
        "  print(xtrain[22400])\n",
        "except:\n",
        "  print(\"failed\")"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(44800,)\n",
            "failed\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aQIG18cBhJli"
      },
      "source": [
        "##The SVM Part"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "C_KYuNGpzM-4"
      },
      "source": [
        "in the following lines we implemented the SVM classifier class and the fit function to train it  then create an instance of the classifier and train it on our one hot or Glove represented dataset on both one hot embedding and Glove embedding with mini batch gradient descent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2GhPl_gCPFP8",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 979
        },
        "outputId": "91d375f2-d40c-4a76-a6ab-268af8521593"
      },
      "source": [
        "##SVM implementation\n",
        "class Svm_classifier(nn.Module):\n",
        "  \"\"\"\n",
        "  We consider the SVM as a onle layer perceptron with vector W for weights and scalar b \n",
        "  \"\"\"\n",
        "  def __init__(self, n_features = len(word2id)):\n",
        "    super().__init__()\n",
        "    print(n_features)\n",
        "    self.mod_par = nn.Linear(n_features , 1)\n",
        "  def forward(self ,x):\n",
        "    return self.mod_par(x)\n",
        "\n",
        "\n",
        "def fitsvm (model , X_train , Y_train  , embedding, batch_size=10 , epochs=1 , learning_rate =0.01 , verbose =True):\n",
        "\n",
        "    \"\"\"\n",
        "    this function takes The training X and Y and fits the svm \n",
        "    to the data using the gradiant decent and the loss of the svm \n",
        "   \"\"\"\n",
        "\n",
        "    optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
        "    model.train()\n",
        "    datasize = X_train.shape[0]-1\n",
        "    for epoch in range(epochs):\n",
        "      \n",
        "      indexes = [i for i in range(len(X_train))]\n",
        "      # shuffle(indexes)\n",
        "      sum_loss = 0  \n",
        "      for start in range(0 , datasize , batch_size):\n",
        "          if start %500 ==0 :\n",
        "              print(f\"I am on test number {start}\")\n",
        "          xreview =[]\n",
        "          yrating =[]\n",
        "          ids = indexes[start : start+ batch_size]\n",
        "          for id in ids:\n",
        "              # print(id , X_train[id])\n",
        "            try :\n",
        "              xreview.append(get_one_hot(X_train[id]))\n",
        "              yrating.append([Y_train[id]])\n",
        "            except :\n",
        "              continue \n",
        "\n",
        "          # x = [embedding(xr) for xr in xreview]\n",
        "          x= torch.tensor(xreview)\n",
        "          y= torch.tensor(yrating)\n",
        "          \n",
        "          \n",
        "          if verbose :\n",
        "            print(x)\n",
        "            print(y)\n",
        "            \n",
        "          # break\n",
        "\n",
        "          x = Variable(x)  \n",
        "          y = Variable(y)\n",
        "          optimizer.zero_grad() \n",
        "          yhat = model(x)  \n",
        "          if verbose :\n",
        "            print(yhat)\n",
        "            verbose = False \n",
        "          loss = torch.mean(torch.clamp(1 - yhat * y, min=0))\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "          sum_loss+=loss.data.cpu().numpy()\n",
        "      print(\"Epoch {}, Loss: {}\".format(epoch, sum_loss))\n",
        "    return model\n",
        "\n",
        "onehot_svm = Svm_classifier()\n",
        "SVM_OH =fitsvm(onehot_svm , xtrain , ytrain , get_one_hot )\n",
        "glove_svm =Svm_classifier(100)\n",
        "SVM_GLOVE =fitsvm(glove_svm , xtrain , ytrain , get_Glove_vec )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "61849\n",
            "I am on test number 0\n",
            "tensor([[1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 1.,  ..., 0., 0., 0.]])\n",
            "tensor([[-1],\n",
            "        [-1],\n",
            "        [-1],\n",
            "        [-1],\n",
            "        [-1],\n",
            "        [-1],\n",
            "        [-1],\n",
            "        [-1],\n",
            "        [-1],\n",
            "        [-1]])\n",
            "tensor([[ 0.0093],\n",
            "        [-0.0017],\n",
            "        [-0.0010],\n",
            "        [ 0.0055],\n",
            "        [-0.0077],\n",
            "        [ 0.0225],\n",
            "        [ 0.0060],\n",
            "        [ 0.0136],\n",
            "        [-0.0120],\n",
            "        [ 0.0074]], grad_fn=<AddmmBackward>)\n",
            "I am on test number 500\n",
            "I am on test number 1000\n",
            "I am on test number 1500\n",
            "I am on test number 2000\n",
            "I am on test number 2500\n",
            "I am on test number 3000\n",
            "I am on test number 3500\n",
            "I am on test number 4000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-36-d2ea96a62720>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0monehot_svm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSvm_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m \u001b[0mSVM_OH\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mfitsvm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0monehot_svm\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mxtrain\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mytrain\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mget_one_hot\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0mglove_svm\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mSvm_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0mSVM_GLOVE\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mfitsvm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglove_svm\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mxtrain\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mytrain\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mget_Glove_vec\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-36-d2ea96a62720>\u001b[0m in \u001b[0;36mfitsvm\u001b[0;34m(model, X_train, Y_train, embedding, batch_size, epochs, learning_rate, verbose)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m           \u001b[0;31m# x = [embedding(xr) for xr in xreview]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m           \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxreview\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m           \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myrating\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xzpIef30nloA"
      },
      "source": [
        "## NN algorithm "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bgTLDjvCz6r9"
      },
      "source": [
        "this is NN classifier implementation as NN_Review_classifier and its fit function to do training on our one hot or Glove represented dataset on both one hot embedding and Glove embedding with mini batch gradient descent"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 996
        },
        "id": "2X2kDAUTnkaB",
        "outputId": "92b50f77-8fbc-492c-a672-8439ad095e49"
      },
      "source": [
        "class NN_Review_classifier(nn.Module):\n",
        "\n",
        "  def __init__(self, n_features = len(word2id)):\n",
        "    super().__init__()\n",
        "    print(n_features)\n",
        "    self.N1 =1024\n",
        "    self.N2 =512\n",
        "    self.layer1 = nn.Linear(n_features , N1)\n",
        "    self.layer2 = nn.Linear(self.N1, self.N2)\n",
        "    self.layer3 = nn.Linear(self.N2 , 1)\n",
        "\n",
        "\n",
        "  def forward(self ,x):\n",
        "    out = self.layer1(x)\n",
        "    out = self.layer2(out)\n",
        "    out = self.layer3(out)\n",
        "    return out\n",
        "\n",
        "\n",
        "def fit_NN (modelx , X_train , Y_train  , embedding, batch_size=10 , epochs=1 , learning_rate =0.01 , verbose =True):\n",
        "\n",
        "    \"\"\"\n",
        "    this function takes The training X and Y and fits the NN \n",
        "    to the data using the gradiant decent and the loss of the NN \n",
        "    \n",
        "   \"\"\"\n",
        "\n",
        "    optimizer = optim.SGD(modelx.parameters(), lr=learning_rate)\n",
        "    \n",
        "    modelx.train()\n",
        "    loss_func = nn.BCEWithLogitsLoss()\n",
        "    datasize = X_train.shape[0]\n",
        "    for epoch in range(epochs):\n",
        "      \n",
        "      indexes = [i for i in range(len(X_train))]\n",
        "      # shuffle(indexes)\n",
        "      sum_loss = 0  \n",
        "      for start in range(0 , datasize , batch_size):\n",
        "          xreview =[]\n",
        "          yrating =[]\n",
        "          ids = indexes[start : start+ batch_size]\n",
        "          for id in ids:\n",
        "              # print(id , X_train[id])\n",
        "              xreview.append(embedding(X_train[id]))\n",
        "              yrating.append([Y_train[id]])\n",
        "\n",
        "          # x = [embedding(xr) for xr in xreview]\n",
        "          x= torch.tensor(xreview)\n",
        "          y= torch.tensor(yrating)\n",
        "          \n",
        "          \n",
        "          if verbose :\n",
        "            print(x)\n",
        "            print(y)\n",
        "            \n",
        "          # break\n",
        "\n",
        "          x = Variable(x)  \n",
        "          y = Variable(y)\n",
        "          optimizer.zero_grad() \n",
        "          yhat = modelx(x)  \n",
        "          if verbose :\n",
        "            print(yhat)\n",
        "            verbose = False \n",
        "          loss = loss_func(yhat, y.float())\n",
        "          loss.backward()\n",
        "          optimizer.step()\n",
        "          sum_loss+=loss.data.cpu().numpy()\n",
        "          if start %500 ==0 :\n",
        "            print(f\"I am on test number {start}\")\n",
        "      print(\"Epoch {}, Loss: {}\".format(epoch, sum_loss))\n",
        "    return model\n",
        "\n",
        "model = NN_Review_classifier(len(word2id))\n",
        "NN_OH =fit_NN(model , xtrain , ytrain , get_one_hot )\n",
        "madel2=NN_Review_classifier(n_features=100)\n",
        "NN_GLOVE =fit_NN(model2 , xtrain , ytrain , get_Glove_vec )"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "61849\n",
            "tensor([[1., 1., 1.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
            "        ...,\n",
            "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
            "        [0., 0., 1.,  ..., 0., 0., 0.]])\n",
            "tensor([[-1],\n",
            "        [-1],\n",
            "        [-1],\n",
            "        [-1],\n",
            "        [-1],\n",
            "        [-1],\n",
            "        [-1],\n",
            "        [-1],\n",
            "        [-1],\n",
            "        [-1]])\n",
            "tensor([[ 0.0086],\n",
            "        [-0.0053],\n",
            "        [ 0.0205],\n",
            "        [ 0.0219],\n",
            "        [ 0.0363],\n",
            "        [ 0.0356],\n",
            "        [-0.0099],\n",
            "        [-0.0014],\n",
            "        [ 0.0053],\n",
            "        [ 0.0058]], grad_fn=<AddmmBackward>)\n",
            "I am on test number 0\n",
            "I am on test number 500\n",
            "I am on test number 1000\n",
            "I am on test number 1500\n",
            "I am on test number 2000\n",
            "I am on test number 2500\n",
            "I am on test number 3000\n",
            "I am on test number 3500\n",
            "I am on test number 4000\n",
            "I am on test number 4500\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-37-121a439740ca>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     64\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     65\u001b[0m \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNN_Review_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mword2id\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 66\u001b[0;31m \u001b[0mNN_OH\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mfit_NN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mxtrain\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mytrain\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mget_one_hot\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     67\u001b[0m \u001b[0mmadel2\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mNN_Review_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mn_features\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0mNN_GLOVE\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mfit_NN\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel2\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mxtrain\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mytrain\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mget_Glove_vec\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-37-121a439740ca>\u001b[0m in \u001b[0;36mfit_NN\u001b[0;34m(modelx, X_train, Y_train, embedding, batch_size, epochs, learning_rate, verbose)\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     38\u001b[0m           \u001b[0;31m# x = [embedding(xr) for xr in xreview]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 39\u001b[0;31m           \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxreview\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     40\u001b[0m           \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myrating\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    }
  ]
}