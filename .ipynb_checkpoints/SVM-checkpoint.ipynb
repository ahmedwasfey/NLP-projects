{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Kg52dVNN_6Ue"
   },
   "source": [
    "##Data reading and Preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Hd6iOl4ZuH77",
    "outputId": "d4d30ba3-a27f-4622-92dd-00d4568767ba"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trying to fetch /content/data/yelp/raw_train.csv\n",
      "12536it [00:04, 2783.52it/s]\n",
      "Trying to fetch /content/data/yelp/raw_test.csv\n",
      "848it [00:00, 2772.48it/s]\n",
      "Trying to fetch /content/data/yelp/reviews_with_splits_lite.csv\n",
      "1217it [00:00, 4105.41it/s]\n",
      "Trying to fetch /content/data/surnames/surnames.csv\n",
      "6it [00:00, 4662.93it/s]\n",
      "Trying to fetch /content/data/surnames/surnames_with_splits.csv\n",
      "8it [00:00, 3263.41it/s]\n",
      "Trying to fetch /content/data/books/frankenstein.txt\n",
      "14it [00:00, 5349.88it/s]\n",
      "Trying to fetch /content/data/books/frankenstein_with_splits.csv\n",
      "109it [00:00, 3422.95it/s]\n",
      "Trying to fetch /content/data/ag_news/news.csv\n",
      "188it [00:00, 1729.55it/s]\n",
      "Trying to fetch /content/data/ag_news/news_with_splits.csv\n",
      "208it [00:00, 2457.82it/s]\n",
      "Trying to fetch /content/data/nmt/eng-fra.txt\n",
      "292it [00:00, 1518.26it/s]\n",
      "Trying to fetch /content/data/nmt/simplest_eng_fra.csv\n",
      "30it [00:00, 5433.97it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 1,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": [
    "%%shell \n",
    "cd data/\n",
    "bash get-all-data.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "RSolp3wm00Kq",
    "outputId": "7f994e9f-07c8-4c50-c905-1f8782bc64a9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/content\n"
     ]
    }
   ],
   "source": [
    "cd /content/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "sjZotSPrwMvl",
    "outputId": "48d726a8-34d4-4466-9281-596cc5adaf2c"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   rating                                             review\n",
      "0       1  Unfortunately, the frustration of being Dr. Go...\n",
      "1       2  Been going to Dr. Goldberg for over 10 years. ...\n",
      "2       1  I don't know what Dr. Goldberg was like before...\n",
      "3       1  I'm writing this review to give you a heads up...\n",
      "4       2  All the food is great here. But the best thing...\n"
     ]
    }
   ],
   "source": [
    "##Pre-Processing \n",
    "import collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "\n",
    "raw_train_dataset_csv=r\"data/yelp/raw_train.csv\"\n",
    "raw_test_dataset_csv=r\"data/yelp/raw_test.csv\"\n",
    "proportion_subset_of_train=0.1\n",
    "train_proportion=0.8\n",
    "test_proportion=0.2\n",
    "output_csv=r\"data/yelp/reviews_with_splits_lite.csv\"\n",
    "seed=1337\n",
    "\n",
    "\n",
    "train_reviews = pd.read_csv(raw_train_dataset_csv, header=None, names=['rating', 'review'])\n",
    "print(train_reviews.head())\n",
    "\n",
    "by_rating = collections.defaultdict(list)\n",
    "for _, row in train_reviews.iterrows():\n",
    "    by_rating[row.rating].append(row.to_dict())\n",
    "    # print(row.to_dict())\n",
    "    # break\n",
    "    \n",
    "review_subset = []\n",
    "\n",
    "for _ , item_list in sorted(by_rating.items()):\n",
    "\n",
    "    n_total = len(item_list)\n",
    "    n_subset = int(proportion_subset_of_train * n_total)\n",
    "    review_subset.extend(item_list[:n_subset])\n",
    "    # print(f\"at { _ } , n total ={n_total} , n subset ={n_subset} ,\\n\")# item list ={item_list}\\n=====\\n\\n\")\n",
    "    \n",
    "\n",
    "review_subset = pd.DataFrame(review_subset)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "WEHBxffi-hFS"
   },
   "outputs": [],
   "source": [
    "by_rating = collections.defaultdict(list)\n",
    "for _, row in review_subset.iterrows():\n",
    "    by_rating[row.rating].append(row.to_dict())\n",
    "    \n",
    "final_list = []\n",
    "np.random.seed(seed)\n",
    "\n",
    "for _, item_list in sorted(by_rating.items()):\n",
    "\n",
    "    np.random.shuffle(item_list)\n",
    "    \n",
    "    n_total = len(item_list)\n",
    "    n_train = int(train_proportion * n_total)\n",
    "    n_test = int(test_proportion * n_total)\n",
    "    \n",
    "    # Give data point a split attribute\n",
    "    for item in item_list[:n_train]:\n",
    "        item['split'] = 'train'\n",
    "  \n",
    "    for item in item_list[n_train:n_train+n_test]:\n",
    "        item['split'] = 'test'\n",
    "\n",
    "    # Add to final list\n",
    "    final_list.extend(item_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Qc2TW6rk-7dy"
   },
   "outputs": [],
   "source": [
    "final_reviews = pd.DataFrame(final_list)\n",
    "def preprocess_text(text):\n",
    "    text = text.lower()\n",
    "    text = re.sub(r\"([.,!?])\", r\" \\1 \", text)\n",
    "    text = re.sub(r\"[^a-zA-Z.,!?]+\", r\" \", text)\n",
    "    return text\n",
    "    \n",
    "final_reviews.review = final_reviews.review.apply(preprocess_text)\n",
    "final_reviews['rating'] = final_reviews.rating.apply({1: -1, 2: 1}.get)\n",
    "final_reviews.to_csv(r\"data/yelp/reviews_with_splits_lite_new.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3U17XZxDAAW_"
   },
   "source": [
    "## sloution starts from here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "AwvWxvlFAHYI",
    "outputId": "e42932cb-be73-4b46-d6d5-c5d2ed36fef3"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train    44800\n",
      "test     11200\n",
      "Name: split, dtype: int64\n",
      "   rating                                             review  split\n",
      "0      -1  terrible place to work for i just heard a stor...  train\n",
      "1      -1   hours , minutes total time for an extremely s...  train\n",
      "2      -1  my less than stellar review is for service . w...  train\n",
      "3      -1  i m granting one star because there s no way t...  train\n",
      "4      -1  the food here is mediocre at best . i went aft...  train\n"
     ]
    }
   ],
   "source": [
    "import collections\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sklearn \n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "from random import  shuffle \n",
    "\n",
    "DATA_PATH = r'data/yelp/reviews_with_splits_lite_new.csv'\n",
    "\n",
    "AllData = pd.read_csv(DATA_PATH)\n",
    "print(AllData.split.value_counts())\n",
    "print(AllData.head())\n",
    "\n",
    "\n",
    "my_vocab =[]\n",
    "word2id ={}\n",
    "id2word ={}\n",
    "\n",
    "\n",
    "def prepare():\n",
    "  \"\"\" read all reviews and add every word of all reviews to the \n",
    "  vocabulary 'word2id' and 'id2word' dictionaries \"\"\"\n",
    "  for review in AllData['review']:\n",
    "    for word in review.split(\" \"):\n",
    "      if word not in word2id :\n",
    "        index = len(word2id)\n",
    "        word2id[word]=index\n",
    "        id2word[index]= word\n",
    "        \n",
    "\n",
    "def get_one_hot(review):\n",
    "        \"\"\"Create a collapsed one-hot vector for the review\n",
    "        Args:\n",
    "            review (str): the review \n",
    "        Returns:\n",
    "            one_hot (np.ndarray): the collapsed one-hot encoding \n",
    "        \"\"\"\n",
    "        one_hot = np.zeros(len(word2id), dtype=np.float32)\n",
    "        for token in review.split(\" \"):\n",
    "                one_hot[word2id[token]] = 1\n",
    "        return one_hot\n",
    "\n",
    "def transform_df(review_df):\n",
    "  one_hot=np.empty([len(review_df) , len(word2id)])\n",
    "  for i in range(len(review_df)):\n",
    "    one_hot[i] =(get_one_hot(review_df[i]))\n",
    "  one_hot_df = pd.DataFrame()\n",
    "  one_hot_df['review'] =pd.Series(one_hot)\n",
    "  return one_hot_df\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "bThZE3V_kdka",
    "outputId": "19e00f6e-1d8a-49a9-d230-ee5d7ed332e2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2021-03-04 08:54:03--  http://nlp.stanford.edu/data/glove.6B.zip\n",
      "Resolving nlp.stanford.edu (nlp.stanford.edu)... 171.64.67.140\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:80... connected.\n",
      "HTTP request sent, awaiting response... 302 Found\n",
      "Location: https://nlp.stanford.edu/data/glove.6B.zip [following]\n",
      "--2021-03-04 08:54:03--  https://nlp.stanford.edu/data/glove.6B.zip\n",
      "Connecting to nlp.stanford.edu (nlp.stanford.edu)|171.64.67.140|:443... connected.\n",
      "HTTP request sent, awaiting response... 301 Moved Permanently\n",
      "Location: http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip [following]\n",
      "--2021-03-04 08:54:03--  http://downloads.cs.stanford.edu/nlp/data/glove.6B.zip\n",
      "Resolving downloads.cs.stanford.edu (downloads.cs.stanford.edu)... 171.64.64.22\n",
      "Connecting to downloads.cs.stanford.edu (downloads.cs.stanford.edu)|171.64.64.22|:80... connected.\n",
      "HTTP request sent, awaiting response... 200 OK\n",
      "Length: 862182613 (822M) [application/zip]\n",
      "Saving to: ‘glove.6B.zip’\n",
      "\n",
      "glove.6B.zip        100%[===================>] 822.24M  1.95MB/s    in 6m 53s  \n",
      "\n",
      "2021-03-04 09:00:55 (1.99 MB/s) - ‘glove.6B.zip’ saved [862182613/862182613]\n",
      "\n",
      "Archive:  glove.6B.zip\n",
      "  inflating: glove.6B.50d.txt        \n",
      "  inflating: glove.6B.100d.txt       \n",
      "  inflating: glove.6B.200d.txt       \n",
      "  inflating: glove.6B.300d.txt       \n",
      "Loading Glove Model\n"
     ]
    }
   ],
   "source": [
    "#download Glove\n",
    "!wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "!unzip glove*.zip\n",
    "gloveModel = {}\n",
    "f = open('glove.6B.100d.txt', encoding='utf-8')\n",
    "\n",
    "print(\"Loading Glove Model\")\n",
    "for line in f:\n",
    "  splitLines = line.split()\n",
    "  word = splitLines[0]\n",
    "  wordEmbedding = np.array([float(value) for value in splitLines[1:]])\n",
    "  gloveModel[word] = wordEmbedding\n",
    "\n",
    "def get_Glove_vec(sent):\n",
    "  embed = {}\n",
    "  words=sent.split()\n",
    "  sent_vect=(np.mean([gloveModel[word] for word in words], axis=0)).tolist()\n",
    "  return Glove_vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Q-eCOs2wmbu0"
   },
   "outputs": [],
   "source": [
    "\n",
    "prepare()\n",
    "dtrain = AllData[AllData['split']=='train']\n",
    "dtest = AllData[AllData['split']=='test']\n",
    "\n",
    "xtrain , ytrain = dtrain['review'] , dtrain['rating']\n",
    "xtest  , ytest = dtest['review'] , dtest['rating']\n",
    "xsample , ysample = xtrain[:20] , ytrain[:20]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-6HV_EBtgYcr",
    "outputId": "d4022213-ca20-44f5-b7cc-f23c862362ae"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(44800,)\n",
      "failed\n"
     ]
    }
   ],
   "source": [
    "print(xtrain.shape)\n",
    "try :\n",
    "  print(xtrain[22400])\n",
    "except:\n",
    "  print(\"failed\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aQIG18cBhJli"
   },
   "source": [
    "##The SVM Part"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "2GhPl_gCPFP8",
    "outputId": "f4f73995-9f0e-44f0-90b0-5defbd5b89f9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "61849\n",
      "I am on test number 0\n",
      "tensor([[1., 1., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.],\n",
      "        ...,\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 1., 0.,  ..., 0., 0., 0.],\n",
      "        [0., 0., 1.,  ..., 0., 0., 0.]])\n",
      "tensor([[-1],\n",
      "        [-1],\n",
      "        [-1],\n",
      "        [-1],\n",
      "        [-1],\n",
      "        [-1],\n",
      "        [-1],\n",
      "        [-1],\n",
      "        [-1],\n",
      "        [-1]])\n",
      "tensor([[-0.0282],\n",
      "        [ 0.0023],\n",
      "        [ 0.0029],\n",
      "        [-0.0188],\n",
      "        [-0.0027],\n",
      "        [ 0.0191],\n",
      "        [-0.0085],\n",
      "        [-0.0024],\n",
      "        [ 0.0157],\n",
      "        [ 0.0153]], grad_fn=<AddmmBackward>)\n",
      "I am on test number 500\n",
      "I am on test number 1000\n",
      "I am on test number 1500\n",
      "I am on test number 2000\n",
      "I am on test number 2500\n",
      "I am on test number 3000\n",
      "I am on test number 3500\n",
      "I am on test number 4000\n",
      "I am on test number 4500\n",
      "I am on test number 5000\n",
      "I am on test number 5500\n",
      "I am on test number 6000\n",
      "I am on test number 6500\n",
      "I am on test number 7000\n",
      "I am on test number 7500\n",
      "I am on test number 8000\n",
      "I am on test number 8500\n",
      "I am on test number 9000\n",
      "I am on test number 9500\n",
      "I am on test number 10000\n",
      "I am on test number 10500\n",
      "I am on test number 11000\n",
      "I am on test number 11500\n",
      "I am on test number 12000\n",
      "I am on test number 12500\n",
      "I am on test number 13000\n",
      "I am on test number 13500\n",
      "I am on test number 14000\n",
      "I am on test number 14500\n",
      "I am on test number 15000\n",
      "I am on test number 15500\n",
      "I am on test number 16000\n",
      "I am on test number 16500\n",
      "I am on test number 17000\n",
      "I am on test number 17500\n",
      "I am on test number 18000\n",
      "I am on test number 18500\n",
      "I am on test number 19000\n",
      "I am on test number 19500\n",
      "I am on test number 20000\n",
      "I am on test number 20500\n",
      "I am on test number 21000\n",
      "I am on test number 21500\n",
      "I am on test number 22000\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "ignored",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-28-d2ea96a62720>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     67\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0monehot_svm\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSvm_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 69\u001b[0;31m \u001b[0mSVM_OH\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mfitsvm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0monehot_svm\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mxtrain\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mytrain\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mget_one_hot\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     70\u001b[0m \u001b[0mglove_svm\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mSvm_classifier\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     71\u001b[0m \u001b[0mSVM_GLOVE\u001b[0m \u001b[0;34m=\u001b[0m\u001b[0mfitsvm\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mglove_svm\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mxtrain\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mytrain\u001b[0m \u001b[0;34m,\u001b[0m \u001b[0mget_Glove_vec\u001b[0m \u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-28-d2ea96a62720>\u001b[0m in \u001b[0;36mfitsvm\u001b[0;34m(model, X_train, Y_train, embedding, batch_size, epochs, learning_rate, verbose)\u001b[0m\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m           \u001b[0;31m# x = [embedding(xr) for xr in xreview]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m           \u001b[0mx\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mxreview\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m           \u001b[0my\u001b[0m\u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0myrating\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "##SVM implementation\n",
    "class Svm_classifier(nn.Module):\n",
    "  \"\"\"\n",
    "  We consider the SVM as a onle layer perceptron with vector W for weights and scalar b \n",
    "  \"\"\"\n",
    "  def __init__(self, n_features = len(word2id)):\n",
    "    super().__init__()\n",
    "    print(n_features)\n",
    "    self.mod_par = nn.Linear(n_features , 1)\n",
    "  def forward(self ,x):\n",
    "    return self.mod_par(x)\n",
    "\n",
    "\n",
    "def fitsvm (model , X_train , Y_train  , embedding, batch_size=10 , epochs=1 , learning_rate =0.01 , verbose =True):\n",
    "\n",
    "    \"\"\"\n",
    "    this function takes The training X and Y and fits the svm \n",
    "    to the data using the gradiant decent and the loss of the svm \n",
    "   \"\"\"\n",
    "\n",
    "    optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "    model.train()\n",
    "    datasize = X_train.shape[0]-1\n",
    "    for epoch in range(epochs):\n",
    "      \n",
    "      indexes = [i for i in range(len(X_train))]\n",
    "      # shuffle(indexes)\n",
    "      sum_loss = 0  \n",
    "      for start in range(0 , datasize , batch_size):\n",
    "          if start %500 ==0 :\n",
    "              print(f\"I am on test number {start}\")\n",
    "          xreview =[]\n",
    "          yrating =[]\n",
    "          ids = indexes[start : start+ batch_size]\n",
    "          for id in ids:\n",
    "              # print(id , X_train[id])\n",
    "            try :\n",
    "              xreview.append(get_one_hot(X_train[id]))\n",
    "              yrating.append([Y_train[id]])\n",
    "            except :\n",
    "              continue \n",
    "\n",
    "          # x = [embedding(xr) for xr in xreview]\n",
    "          x= torch.tensor(xreview)\n",
    "          y= torch.tensor(yrating)\n",
    "          \n",
    "          \n",
    "          if verbose :\n",
    "            print(x)\n",
    "            print(y)\n",
    "            \n",
    "          # break\n",
    "\n",
    "          x = Variable(x)  \n",
    "          y = Variable(y)\n",
    "          optimizer.zero_grad() \n",
    "          yhat = model(x)  \n",
    "          if verbose :\n",
    "            print(yhat)\n",
    "            verbose = False \n",
    "          loss = torch.mean(torch.clamp(1 - yhat * y, min=0))\n",
    "          loss.backward()\n",
    "          optimizer.step()\n",
    "          sum_loss+=loss.data.cpu().numpy()\n",
    "      print(\"Epoch {}, Loss: {}\".format(epoch, sum_loss))\n",
    "    return model\n",
    "\n",
    "onehot_svm = Svm_classifier()\n",
    "SVM_OH =fitsvm(onehot_svm , xtrain , ytrain , get_one_hot )\n",
    "glove_svm =Svm_classifier(100)\n",
    "SVM_GLOVE =fitsvm(glove_svm , xtrain , ytrain , get_Glove_vec )"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Task1.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
